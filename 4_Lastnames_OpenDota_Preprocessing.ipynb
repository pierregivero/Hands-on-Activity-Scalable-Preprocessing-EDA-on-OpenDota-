{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06272088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4faaa027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SparkSession to stop (ok).\n",
      "✅ PySpark internals reset\n",
      "✅ Spark started: 4.1.1\n",
      " - Master: local[*]\n",
      " - Spark UI: http://LAPTOP-GGN1Q8MB:4041\n",
      "alive_check = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Stop any existing SparkSession (if any) ----------\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped existing SparkSession.\")\n",
    "except Exception:\n",
    "    print(\"No SparkSession to stop (ok).\")\n",
    "\n",
    "# ---------- 2) Clear PySpark global state (fixes dead Py4J gateway) ----------\n",
    "try:\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    for k in [\"PYSPARK_GATEWAY_PORT\", \"PYSPARK_GATEWAY_SECRET\"]:\n",
    "        if k in os.environ:\n",
    "            os.environ.pop(k, None)\n",
    "            print(f\"Cleared env {k}\")\n",
    "\n",
    "    SparkContext._active_spark_context = None\n",
    "    SparkContext._gateway = None\n",
    "    SparkContext._jvm = None\n",
    "\n",
    "    try:\n",
    "        SparkSession._instantiatedContext = None\n",
    "        SparkSession._activeSession = None\n",
    "        SparkSession._defaultSession = None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"✅ PySpark internals reset\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Reset step warning:\", type(e).__name__, e)\n",
    "\n",
    "# ---------- 3) Start Spark (stable local mode) ----------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "OUTPUT_ROOT = Path.home() / \"opendota_processed\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPARK_LOCAL_DIR = OUTPUT_ROOT / \"spark_local\"\n",
    "SPARK_LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"OpenDota_Notebook\")\n",
    "    .master(\"local[*]\")  # stable; we can change later\n",
    "    .config(\"spark.local.dir\", str(SPARK_LOCAL_DIR))\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    # IMPORTANT: avoid surprise broadcasts that can destabilize the JVM\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark started:\", spark.version)\n",
    "print(\" - Master:\", spark.sparkContext.master)\n",
    "print(\" - Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"alive_check =\", spark.range(1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670daeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing SparkSession.\n",
      "✅ PySpark internals reset\n",
      "✅ Spark started: 4.1.1\n",
      " - Master: local[*]\n",
      " - Spark UI: http://LAPTOP-GGN1Q8MB:4041\n",
      "alive_check = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Stop any existing SparkSession (if any) ----------\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped existing SparkSession.\")\n",
    "except Exception:\n",
    "    print(\"No SparkSession to stop (ok).\")\n",
    "\n",
    "# ---------- 2) Clear PySpark global state (fixes dead Py4J gateway) ----------\n",
    "try:\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    for k in [\"PYSPARK_GATEWAY_PORT\", \"PYSPARK_GATEWAY_SECRET\"]:\n",
    "        if k in os.environ:\n",
    "            os.environ.pop(k, None)\n",
    "            print(f\"Cleared env {k}\")\n",
    "\n",
    "    SparkContext._active_spark_context = None\n",
    "    SparkContext._gateway = None\n",
    "    SparkContext._jvm = None\n",
    "\n",
    "    try:\n",
    "        SparkSession._instantiatedContext = None\n",
    "        SparkSession._activeSession = None\n",
    "        SparkSession._defaultSession = None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"✅ PySpark internals reset\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Reset step warning:\", type(e).__name__, e)\n",
    "\n",
    "# ---------- 3) Start Spark (stable local mode) ----------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "OUTPUT_ROOT = Path.home() / \"opendota_processed\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPARK_LOCAL_DIR = OUTPUT_ROOT / \"spark_local\"\n",
    "SPARK_LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"OpenDota_Notebook\")\n",
    "    .master(\"local[*]\")  # stable; we can change later\n",
    "    .config(\"spark.local.dir\", str(SPARK_LOCAL_DIR))\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    # IMPORTANT: avoid surprise broadcasts that can destabilize the JVM\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark started:\", spark.version)\n",
    "print(\" - Master:\", spark.sparkContext.master)\n",
    "print(\" - Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"alive_check =\", spark.range(1).count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
